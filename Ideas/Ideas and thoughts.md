
# Using a tensor or outer product to get variable output length

# Sharing Weights is a little like sending our output back +/forth

# For monte carlo loss, why don't we accumulate the sum and at the same time check the variance if it is below some threshold and then accumulate some more if it is not and otherwise do the update step to reduce the noise in our gradient.

# Relabling via model to produce labels and other model to standardize these labels

# So just like we want a linear approximation in our diffusion space for faster sampling, stochastic gradient descent is at it√∏s core the form a of a differential equation, which means if we could have that we can sample parameters in a line, then we will arrive at where we want to go quicker.

