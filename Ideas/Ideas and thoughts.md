
# Using a tensor or outer product to get variable output length

# Sharing Weights is a little like sending our output back +/forth

# For monte carlo loss, why don't we accumulate the sum and at the same time check the variance if it is below some threshold and then accumulate some more if it is not and otherwise do the update step to reduce the noise in our gradient.

